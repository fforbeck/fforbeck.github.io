<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>a bit of bits...</title><link href="http://felipeforbeck.com/" rel="alternate"></link><link href="http://felipeforbeck.com/feeds/felipe-forbeck.atom.xml" rel="self"></link><id>http://felipeforbeck.com/</id><updated>2015-11-05T20:36:00-08:00</updated><entry><title>Elasticsearch - Server and Client config notes</title><link href="http://felipeforbeck.com/posts/2015/11/elasticsearch-notes-about-server-client-configs/" rel="alternate"></link><updated>2015-11-05T20:36:00-08:00</updated><author><name>Felipe Forbeck</name></author><id>tag:felipeforbeck.com,2015-11-05:posts/2015/11/elasticsearch-notes-about-server-client-configs/</id><summary type="html">&lt;h3&gt;ES Server - Elasticsearch v2.0.0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;cluster.routing.allocation.cluster_concurrent_rebalance&lt;/code&gt;: 2&lt;ul&gt;
&lt;li&gt;Determines the number of shards allowed for concurrent rebalance. This property needs to be set appropriately depending on the hardware being used, for example the number of CPUs, IO capacity, etc. If this property is not set appropriately, it can impact the ElasticSearch performance with indexing. By default the value is set at 2, meaning that at any point in time only 2 shards are allowed to be moving. It is good to set this property low so that the rebalance of shards is throttled and doesn't affect indexing.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;index.store.throttle.max_bytes_per_sec&lt;/code&gt;: 10mb&lt;ul&gt;
&lt;li&gt;Allows to control the maximum bytes per sec written to the file system. I was using small documents, something around 10Kb, we can increase this value once we get bigger documents.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;index.number_of_replicas&lt;/code&gt;: 0&lt;ul&gt;
&lt;li&gt;To disable the replica allocation in order to run a single node per cluster. &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;index.routing.allocation.total_shards_per_node&lt;/code&gt;: 2&lt;ul&gt;
&lt;li&gt;The maximum number of shards (replicas and primaries) that will be allocated to a single node. Defaults is unbounded. It imposes a hard limit which can result in some shards not being allocated. Use with caution. It can be changed later to add more shards and help the search feature, because replica shards will receive the queries while primary shards will receive the index and delete requests.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;index.refresh_interval&lt;/code&gt;: 5s&lt;ul&gt;
&lt;li&gt;Better indexing performance if you leave refresh enabled. This is because ES a separate refresh thread which will do the flushing, instead of having your bulk indexing threads to it when RAM is full.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;indices.cluster.send_refresh_mapping&lt;/code&gt;: false&lt;ul&gt;
&lt;li&gt;When the index manager send a node an index request to process, the node updates its own mapping and then sends that mapping to the master. While the master processes it, that node receives a state that includes an older version of the mapping. If there’s a conflict, it’s not bad (i.e. the cluster state will eventually have the correct mapping), but we send a refresh just in case from that node to the master. In order to make the index request more efficient, we have set this property on our data nodes. We are currently running one single node so we do not need to have it enabled.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;node.max_local_storage_nodes&lt;/code&gt;: 1&lt;ul&gt;
&lt;li&gt;Start at most one single node in the cluster.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;action.destructive_requires_name&lt;/code&gt;: true&lt;ul&gt;
&lt;li&gt;The delete index API can also be applied to more than one index, or on all indices by using _all or * as index. To prevent deleting all indices via wildcards or _all.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;ES Java Client - Elasticsearch v2.0.0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;TransportClient&lt;/code&gt; instead of &lt;code&gt;NodeClient&lt;/code&gt;&lt;ul&gt;
&lt;li&gt;Connects to the cluster and does not act like a new node, which reduce the noise in the cluster and allow faster requests.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Parallel bulk requests and do not wait for ES Responses&lt;ul&gt;
&lt;li&gt;ES execute the bulk requests in background and it takes time to send back a response, so just send more bulk requests and put the listeners to parse the responses in another thread.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;BulkProcessorListener&lt;/code&gt; instead of &lt;code&gt;client.prepareBulk()&lt;/code&gt;&lt;ul&gt;
&lt;li&gt;The listener allow you to configure the size of the bulk request and other flush parameters as well.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;BulkProcessorListener&lt;/code&gt; actions: 1000-10000 (current is 5K)&lt;ul&gt;
&lt;li&gt;Do not set more than 10K per bulk actions, it is not recommended.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;BulkProcessorListener&lt;/code&gt; with BulkSize in MB&lt;ul&gt;
&lt;li&gt;Set auto flush to the buffer when it reaches &lt;code&gt;X&lt;/code&gt; MB, even if it contains less than &lt;code&gt;N&lt;/code&gt; actions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;BulkProcessorListener&lt;/code&gt; with concurrentRequests enabled (x &amp;gt;= 1)&lt;ul&gt;
&lt;li&gt;To avoid blocking threads for bulk requests is recommended to set this property using &lt;code&gt;x = 4 * num_available_cores&lt;/code&gt; for the concurrency.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary><category term="elasticsearch"></category><category term="configs"></category><category term="java"></category></entry><entry><title>LOL - who needs help?</title><link href="http://felipeforbeck.com/posts/2015/10/lol-who-needs-help/" rel="alternate"></link><updated>2015-10-05T20:35:00-07:00</updated><author><name>Felipe Forbeck</name></author><id>tag:felipeforbeck.com,2015-10-05:posts/2015/10/lol-who-needs-help/</id><summary type="html">&lt;p&gt;&lt;img alt="Results" src="/images/crazy-dev.gif" /&gt;&lt;/p&gt;</summary><category term="programmer"></category><category term="gif"></category><category term="lol"></category></entry><entry><title>Free CDN Provider - Couldflare</title><link href="http://felipeforbeck.com/posts/2015/09/free-cdn-provider-cloudflare/" rel="alternate"></link><updated>2015-09-05T20:35:00-07:00</updated><author><name>Felipe Forbeck</name></author><id>tag:felipeforbeck.com,2015-09-05:posts/2015/09/free-cdn-provider-cloudflare/</id><summary type="html">&lt;div class="vimeo" align="center"&gt;
    &lt;iframe width="800" height="500" src="https://player.vimeo.com/video/153411340" frameborder="0"&gt;&lt;/iframe&gt;
&lt;/div&gt;</summary><category term="cloudflare"></category><category term="tool"></category><category term="free"></category></entry><entry><title>HP Haven Twitter Analysis Tutorial Challenge - Results</title><link href="http://felipeforbeck.com/posts/2015/02/topcoder-challenge-tweet-analysis/" rel="alternate"></link><updated>2015-02-14T08:22:00-08:00</updated><author><name>Felipe Forbeck</name></author><id>tag:felipeforbeck.com,2015-02-14:posts/2015/02/topcoder-challenge-tweet-analysis/</id><summary type="html">&lt;p&gt;\o/ I got first place with my &lt;a href="https://github.com/fforbeck/twitter-analyst"&gt;App&lt;/a&gt; :D&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.topcoder.com/challenge-details/30048480/#winner"&gt;Results&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Results" src="/images/hp-haven-twitter-analysis-tutorial-challenge-results.png" /&gt;&lt;/p&gt;</summary><category term="play framework"></category><category term="hp-idol"></category><category term="sentiment analysis"></category><category term="topcoder"></category><category term="chanllenge"></category></entry><entry><title>Tweet Sentiment Analysis</title><link href="http://felipeforbeck.com/posts/2015/01/tweet-sentiment-analyst/" rel="alternate"></link><updated>2015-01-05T21:31:00-08:00</updated><author><name>Felipe Forbeck</name></author><id>tag:felipeforbeck.com,2015-01-05:posts/2015/01/tweet-sentiment-analyst/</id><summary type="html">&lt;h1&gt;Why&lt;/h1&gt;
&lt;p&gt;Some days ago I enrolled in my first topcoder development challenge and it is basically a tutorial challenge where we need to install HPVertica DB, capture a set of tweets and based on $HPQ tag, use the HP Idol platform, which has an API to perform sentiment analysis over a tweet text content.&lt;/p&gt;
&lt;p&gt;With that in mind I decided to start this challenge with Play Framework to learn a little bit and see how
fast I could create this WebApp from scratch without know the tool. Beside that, I have read about play framework and it seems to be a very powerful tool which I could easily plug different technologies and keep focused in my development. It provides you a sort of minimum viable architecture for your system. In addition, another reason to pick this technology is because I can use Java, which was one of the requirements of the challenge.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;The challenge&lt;/code&gt; - &lt;a href="https://www.topcoder.com/challenge-details/30048480/"&gt;https://www.topcoder.com/challenge-details/30048480/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;"Here are the steps to participate in the HP Haven Twitter Analysis Tutorial challenge:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;You’ll be creating a Java application which accesses Twitter data for the Hewlett Packard stock symbol tag “$HPQ”, performs sentiment analysis on this data, and loads the raw social feed and sentiment data into the Vertica database.  The application should also display some kind of visualization about how sentiment is changing over time or by topic.  The application should extract enough Tweets that the Sentiment Analysis shows some depth/variation – at least 1000 Tweets, but more would even be better.&lt;/li&gt;
&lt;li&gt;You have creative license about what kind of application to create.  You may create a mobile, web, or desktop app.&lt;/li&gt;
&lt;li&gt;Your application should connect to the IDOL OnDemand platform to perform the Sentiment Analysis on the Twitter data related to the tag $HPQ.  The Sentiment Analysis results should be stored in your locally configured version of Vertica.  Sample Java code to connect to IDOL OnDemand is attached to the challenge.  Sample code can also be found on the IDOL OnDemand Community site.&lt;/li&gt;
&lt;li&gt;This is a tutorial challenge. Your code should be clear and well documented.&lt;/li&gt;
&lt;li&gt;You should produce a blog post about your application.&lt;/li&gt;
&lt;li&gt;You should produce a screensharing video which explains your code and how to set up and connect to a Vertica database.&lt;/li&gt;
&lt;li&gt;There should be some kind of visualization in your app which displays the Sentiment Scores related to a topic and/or time dimension.&lt;/li&gt;
&lt;li&gt;We're currently running a Sweepstakes challenge which walks through the Vertica setup on a local VMWare instance.  We're also attaching a Vertica lab manual which describes how to add users, create schemas, and load data into the system.  It assumes, however, that you have the Vertica Virtual Server instance installed and locally available."&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;h1&gt;How&lt;/h1&gt;
&lt;h3&gt;Tools and Frameworks&lt;/h3&gt;
&lt;p&gt;Java; &lt;a href="https://www.playframework.com/"&gt;Play Framework&lt;/a&gt;; &lt;a href="https://spring.io/"&gt;Spring&lt;/a&gt;; &lt;a href="http://projects.spring.io/spring-data/"&gt;Spring Data&lt;/a&gt;; &lt;a href="http://hibernate.org/orm/"&gt;Hibernate&lt;/a&gt;; &lt;a href="https://www.playframework.com/documentation/2.3.x/JavaWebSockets"&gt;WebSockets&lt;/a&gt;; &lt;a href="http://akka.io/"&gt;Akka&lt;/a&gt;; &lt;a href="https://dev.twitter.com"&gt;Twitter4J Search &amp;amp; Stream API&lt;/a&gt;; &lt;a href="http://redis.io/"&gt;Redis&lt;/a&gt;; &lt;a href="http://www.vertica.com/about/"&gt;HPVerticaDB&lt;/a&gt;; &lt;a href="https://www.idolondemand.com/developer/apis"&gt;HPIdol OnDemand API&lt;/a&gt;; &lt;a href="http://getbootstrap.com/"&gt;Twitter Bootstrap 3&lt;/a&gt;; &lt;a href="http://jquery.com/"&gt;JQuery 1.11&lt;/a&gt;; &lt;a href="http://www.highcharts.com/"&gt;Highcharts&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Application design&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Tweet Analyst Design" src="/images/tweet_analyst_system_design.png" /&gt;&lt;/p&gt;
&lt;h3&gt;Screencast&lt;/h3&gt;
&lt;div class="vimeo" align="center"&gt;
    &lt;iframe width="800" height="500" src="https://player.vimeo.com/video/119002875" frameborder="0"&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h3&gt;Design notes&lt;/h3&gt;
&lt;h4&gt;&lt;a href="https://dev.twitter.com"&gt;Twitter API&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;Stream&lt;/code&gt; - &lt;a href="https://dev.twitter.com/streaming/overview"&gt;https://dev.twitter.com/streaming/overview&lt;/a&gt;:
"The set of streaming APIs offered by Twitter give developers low latency access to Twitter’s global stream of Tweet data. A proper implementation of a streaming client will be pushed messages indicating Tweets and other events have occurred, without any of the overhead associated with polling a REST endpoint."
In this case I am using the public stream API, it is enough to receive the tweets in real time.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Search&lt;/code&gt; - &lt;a href="https://dev.twitter.com/rest/public/search"&gt;https://dev.twitter.com/rest/public/search&lt;/a&gt;:
"The Twitter Search API is part of Twitter’s v1.1 REST API. It allows queries against the indices of recent or popular Tweets.."
Here I am just using a &lt;code&gt;GET&lt;/code&gt; method for &lt;code&gt;twitter.com/search?q=%MY_HASH_TAG%&lt;/code&gt; URL to search and grab the tweets with the hashTag param.&lt;/p&gt;
&lt;hr /&gt;
&lt;h4&gt;Twitter4J - &lt;a href="http://twitter4j.org/en/index.html"&gt;http://twitter4j.org/en/index.html&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;It is an unofficial library to connect to twitter APIs. You can check the code examples &lt;a href="http://twitter4j.org/en/code-examples.html"&gt;here&lt;/a&gt; and &lt;a href="https://github.com/yusuke/twitter4j/tree/master/twitter4j-examples/src/main/java/twitter4j/examples"&gt;more code examples&lt;/a&gt;
To connect to API you will need some oauth keys and consumer keys.&lt;/p&gt;
&lt;hr /&gt;
&lt;h4&gt;HPIdol OnDemand API - &lt;a href="https://www.idolondemand.com/"&gt;https://www.idolondemand.com/&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;Setiment Analysis API&lt;/code&gt; - &lt;a href="http://www.idolondemand.com/developer/apis/analyzesentiment#overview"&gt;http://www.idolondemand.com/developer/apis/analyzesentiment#overview&lt;/a&gt;
"The Sentiment Analysis API analyzes text to return the sentiment as positive, negative or neutral. It contains a dictionary of positive and negative words of different types, and defines patterns that describe how to combine these words to form positive and negative phrases."
The hello world in java for HP Idol API can be found &lt;a href="https://www.youtube.com/watch?v=MY-ASbxRJVw"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;h4&gt;Redis - &lt;a href="http://redis.io/"&gt;http://redis.io/&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;Processing Queue&lt;/code&gt;:
This queue keeps all the tweets that are in processing phase. For instance, when we receive a new tweet from Twitter Stream API or when we found a set of tweets in Twitter Search API, they are placed in this queue to be processed later.
Processing action here means: we need to parse the tweets, extract the relevant information and send it to HP IDOL API to do the analysis for each one of them. The result of this phase will be placed in another queue (persistent-queue).
'The tweets are saved as String in redis queue.'&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Persistent Queue&lt;/code&gt;:
This queue keeps all the tweets that were processed, analyzed and now are ready to be stored in VerticaDB with
sentiment score.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Live Tweets Channel&lt;/code&gt;:
This channel publishes all the tweets that were processed and analyzed to the subscribers of this channel. It is just a Redis pubSub system, you can learn more about it &lt;a href="http://redis.io/topics/pubsub"&gt;here&lt;/a&gt;.
In this app it is really useful, cause I would like to see the sentiment analysis in real time for all tweets that comes from Twitter Stream API. So, right after analyze it, I just send the results to this channel which will automatically publish this content to every subscriber. The subscriber in this case is an Akka Actor Reference which was created by a websocket connection request, that happens when you open the open the index page with the live tweets chart.&lt;/p&gt;
&lt;hr /&gt;
&lt;h4&gt;&lt;code&gt;HP Vertica DB&lt;/code&gt; - &lt;a href="https://my.vertica.com/community/"&gt;https://my.vertica.com/community/&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;You can download the HP Vertica community edition &lt;a href="https://my.vertica.com/community/"&gt;here&lt;/a&gt;
It is a relational database optimized for large-scale analytics. "It is uniquely designed using a memory-and-disk balanced distributed compressed columnar paradigm, which makes it exponentially faster than older techniques for modern data analytics workloads. HP Vertica supports a series of built-in analytics libraries like time series and analytics packs for geospatial and sentiment plus additional functions from vendors like SAS. And, it supports analytics written using the R programming language for predictive modeling."
More details about the technology can be found &lt;a href="http://www.vertica.com/wp-content/uploads/2014/05/VerticaOverview.pdf"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;h4&gt;Akka Actor System&lt;/h4&gt;
&lt;p&gt;Akka is a framework which provides the set of right tools to build high-scalable and fault-tolerant systems using a Actor System model. You can easily write parallel, concurrent, event-driven programs. To learn about Akka &lt;a href="http://doc.akka.io/docs/akka/2.1.2/intro/what-is-akka.html"&gt;check it out&lt;/a&gt;.
In this project I decided to use Akka to create microservices that would connect with external API`s. In this case my 4 actors play different roles and they are managed by one supervisor.
The idea here is to have a service manage that would orchestrate the messages and actions in my actor system.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Tweet Receiver&lt;/code&gt;:
An Actor Reference which connects to Twitter Stream API using Twitter4J library and listen to tweets from API.
For each new tweet, the actor parses the content and push the result into redis processing-queue.
After that it sends a 'Read' message to the TweetSupervisor.
Currently I am starting only one instance of this actor for the tag+language that I want to receive the tweets.
Later I can easily change to create one instance per tag or something like it.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Tweet Harvester&lt;/code&gt;:
An Actor Reference which connects to Twitter Search API to perform the tweet search based on the tag+language.
It can easily reach the Twitter requests rate limit, so I place this Actor to run every 15 min whatever happens
with it. This setup can be found in TweetSupervisor that we will cover in the next sections. After parse the tweets found
it sends a 'Read' message to TweetSupervisor.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Tweet Analyzer&lt;/code&gt;:
Another Actor Reference which receives a 'Read' message from TweetSupervisor to start reading tweets from processing
queue to send it to HP Idol Sentiment Analysis API. The requests to HP API are synchronous and each result is parsed, appended to the original tweet, sent to the persistent-queue and published to the live-tweets-channel.
N instances of this actor will be created be the supervisor, after each instance execute the job, it finalizes itself.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Tweet Publisher&lt;/code&gt;:
It s an special Actor Reference which is not managed by TweetSupervisor. I`m using this Actor to
handle the tweets published in live-tweets-channel and send it directly to the client.
Cause in the client side we have a scatter chart which shows in real time the tweet sentiment analysis.
For this, was necessary to open a websocket connection per client, each connection is handled by an Actor Reference (Tweet Publisher) that subscribes the redis channel.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Tweet Supervisor&lt;/code&gt;:
It handles two type of messages, Read and Start. This messages are java objects and when an Actor Reference (instance) receives a message that it can handle, it simply execute some action based on it.
So, when the TweetSupervisor receives a Start message it starts two actor instances; TweetReceiver and TweetHarvester.
If the supervisor receives a Read message, it starts a new instance of TweetAnalyzer actor.&lt;/p&gt;
&lt;hr /&gt;
&lt;h4&gt;Service, Repository &amp;amp; Controller&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;Tweet Service&lt;/code&gt;:
Starts the Akka Actor System with the TweetSupervisor, Runs a scheduled job in order to persist
the analyzed tweets, provide some methods to find tweets in TweetRepository.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Tweet Repository&lt;/code&gt;:
It is a spring data repository which provides a set of actions to be done in the db. For instance, findAll, findBySomeProperty, and so on.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Application Controller&lt;/code&gt;:
Main controller which receives the HTTP requests, handle and return a Result, which can be a simple html page, a WebSocket connection or JSON response. Available Methods:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Live tweets&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;GET        /                      controllers.Application.liveTweets()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Timeline&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;GET        /timeline              controllers.Application.timelineTweets()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pie chart&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;GET        /pie                   controllers.Application.pieTweets()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Web socket connection &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;GET        /ws-tweets             controllers.Application.wsTweets()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tweets by sentiment type&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;GET        /tweets/:sentiment     controllers.Application.tweets(sentiment: String)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tweets statistics&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;GET        /statistics            controllers.Application.statistics()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h3&gt;Application Installation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;OpenJDK1.5&lt;/code&gt;: sudo apt-get install openjdk-7-jdk&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Play Framework Installation&lt;/code&gt; - &lt;a href="https://www.playframework.com/documentation/2.3.x/Installing"&gt; https://www.playframework.com/documentation/2.3.x/Installing&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Import the project - IntelliJIDEA&lt;/code&gt; - &lt;a href="https://confluence.jetbrains.com/display/IntelliJIDEA/Play+Framework+2.0"&gt;https://confluence.jetbrains.com/display/IntelliJIDEA/Play+Framework+2.0&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;HP Vertica DB&lt;/code&gt; - &lt;a href="https://my.vertica.com/docs/5.0/PDF/Installation%20Guide.pdf"&gt;https://my.vertica.com/docs/5.0/PDF/Installation%20Guide.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Redis&lt;/code&gt; - &lt;a href="http://redis.io/topics/quickstart"&gt;http://redis.io/topics/quickstart&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;HP Idol Keys&lt;/code&gt;- &lt;a href="http://idolondemand.topcoder.com/"&gt;http://idolondemand.topcoder.com/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Place your keys in the application.conf file under twitter-analyst/conf/ folder&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;// HP IDOL Platform
hp.idol.analyze.sentiment.uri=&amp;quot;https://api.idolondemand.com/1/api/sync/analyzesentiment/v1?text=%TWEET%&amp;amp;language=%LANG%&amp;amp;apikey=&amp;lt;IDOL_API_KEY_HERE&amp;gt;&amp;quot;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Twitter API Keys&lt;/code&gt; - &lt;a href="http://www.androidhive.info/2012/09/android-twitter-oauth-connect-tutorial/"&gt;http://www.androidhive.info/2012/09/android-twitter-oauth-connect-tutorial/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Place your new keys in the application.conf file twitter-analyst/conf/ folder&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;// Twitter 4J OAuth
twitter.oauth.consumerKey=&amp;quot;&amp;quot;
twitter.oauth.consumerSecret=&amp;quot;&amp;quot;
twitter.oauth.accessToken=&amp;quot;&amp;quot;
twitter.oauth.accessTokenSecret=&amp;quot;&amp;quot;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="http://www.androidhive.info/2012/09/android-twitter-oauth-connect-tutorial/"&gt;this tutorial&lt;/a&gt; shows you how to get
the twitter keys and tokens.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Connecting to Vertica&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Make sure you are using the persistence.xml with PostgreSQLDialect and setting your default schema name, like&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;persistence&lt;/span&gt; &lt;span class="na"&gt;xmlns=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;http://java.sun.com/xml/ns/persistence&amp;quot;&lt;/span&gt;
             &lt;span class="na"&gt;xmlns:xsi=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;http://www.w3.org/2001/XMLSchema-instance&amp;quot;&lt;/span&gt;
             &lt;span class="na"&gt;xsi:schemaLocation=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;http://java.sun.com/xml/ns/persistence http://java.sun.com/xml/ns/persistence/persistence_2_0.xsd&amp;quot;&lt;/span&gt;
             &lt;span class="na"&gt;version=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;2.0&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;

    &lt;span class="nt"&gt;&amp;lt;persistence-unit&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;defaultPersistenceUnit&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;transaction-type=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;RESOURCE_LOCAL&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;provider&amp;gt;&lt;/span&gt;org.hibernate.ejb.HibernatePersistence&lt;span class="nt"&gt;&amp;lt;/provider&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;non-jta-data-source&amp;gt;&lt;/span&gt;DefaultDS&lt;span class="nt"&gt;&amp;lt;/non-jta-data-source&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;properties&amp;gt;&lt;/span&gt;
            &lt;span class="c"&gt;&amp;lt;!--&amp;lt;property name=&amp;quot;hibernate.hbm2ddl.auto&amp;quot; value=&amp;quot;none&amp;quot; /&amp;gt;--&amp;gt;&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;lt;property&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;hibernate.dialect&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;value=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;org.hibernate.dialect.PostgreSQLDialect&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt; // Postgre Dialect
            &lt;span class="nt"&gt;&amp;lt;property&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;show_sql&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;value=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;true&amp;quot;&lt;/span&gt; &lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;lt;property&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;format_sql&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;value=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;true&amp;quot;&lt;/span&gt; &lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;lt;property&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;use_sql_comments&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;value=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;true&amp;quot;&lt;/span&gt; &lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;lt;property&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;hibernate.default_schema&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;value=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;public&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;/properties&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;/persistence-unit&amp;gt;&lt;/span&gt;

&lt;span class="nt"&gt;&amp;lt;/persistence&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;After that, you need to include in the classpath the &lt;em&gt;vertica-jdk5-6.1.0-0.jar&lt;/em&gt; which is located at twitter-analyst/lib/
folder of the project.&lt;/p&gt;
&lt;p&gt;You need to update the Vertica IP address, db-name, schema-name into application.conf file under twitter-analyst/conf/.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;// VerticaDB
db.default.driver=com.vertica.jdbc.Driver //This is the driver class that we use to connect to Vertica.
db.default.url=&amp;quot;jdbc:vertica://&amp;lt;your.db.ip.addr&amp;gt;:5433/&amp;lt;db-name&amp;gt;&amp;quot;
db.default.user=dbadmin
db.default.password=&amp;quot;&amp;quot;
db.default.schema=&amp;quot;&amp;lt;schema-name&amp;gt;&amp;quot;
db.default.jndiName=DefaultDS
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Creating the Model&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Play Framework can apply your sql scripts to the database, however I could not use this feature. I got some issues with
the Postgre dialect here. So, in this case I suggest you to copy the file from&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;/twitter-analyst/conf/evolutions/VMart/ddl/1.sql&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and execute it directly in VerticaDB. Make sure your schema is the same that we use in these files, or just replace it with your schema name.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Loading the Data&lt;/code&gt;
Under the same folder you will find the dml/2.sql which contains 10K+ inserts of tweet with $HPQ tag, you can load it into your DB. Make sure the schema name is right.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Connecting to Redis&lt;/code&gt;
Just place the Redis IP address in the application.conf file&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;// Redis
redis.host=&amp;quot;&amp;lt;your.redis.ip.addr&amp;gt;&amp;quot;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Starting the Application&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After install the VerticaDB, Redis, Generate you keys for Idol and Twitter, Import the project, you just need
to go in &lt;code&gt;Run -&amp;gt; Run Play 2&lt;/code&gt; and wait for the application bootstrap. &lt;/p&gt;
&lt;p&gt;If you do not see any tweets in live tweets chart, just tweet in your own account using the tag $HPQ and you will see it in the live chart. Or just set the property 'tweet.analyst.tags' in application.conf file to use a trend hashtag and you will receive a tsunami of tweets.&lt;/p&gt;
&lt;p&gt;Here we have two examples of positive and negative tweets in the live tweets chart.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Live Positive Tweet" src="/images/live-positive-tweet.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The green dots are all tweets considered positive, which means they received a score &amp;gt; 0 from HP Sentiment Analysis API.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Live Negative Tweet" src="/images/live-negative-tweet.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The red dots are all tweets considered negative, score &amp;lt; 0.&lt;/li&gt;
&lt;li&gt;The gray dots are all tweets considered neutral, score = 0.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To see the sentiment score information and the tweets content you can navigate between the charts Live, Timeline and Pie. Each one of them will be reloaded completely from scratch every time you hit the page. Place the mouse over the dots to see the content.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Live: tweets analyzed in real time and placed in the chart.&lt;/li&gt;
&lt;li&gt;Timeline: all the tweets analyzed and stored in the db can be filtered by date range.&lt;/li&gt;
&lt;li&gt;Pie: Tweets sentiment share.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h3&gt;Final Thoughts&lt;/h3&gt;
&lt;p&gt;It was my first application using Play Framework, Akka and HP Idol API. I have to say it was really easy and fast to setup and create the app. The more important here was that I could use my time to focus on what features I would create instead of spending time on the minimum viable architecture. &lt;/p&gt;
&lt;p&gt;The Akka is a great framework to create microservices without pain. I would like to had developed unit and integration tests, mainly for the actors. Due to the time constraints, unfortunately, I was not able to do it. But I will. In a near future. I do want to see and learn how easy/complex is to test the actors and the message flow.&lt;/p&gt;
&lt;p&gt;The Sentiment API was another great tool. I really want explore the other APIs from HP Idol OnDemand, for sure. In this challenge I could see that we can create nice apps just plugin into different APIs around the world. We can get a ton of data from social media APIs (Twitter Stream, awesome!! your app receiving tweets near real time). Grab and send it to be processed by whatever open and free service you find out on the internet. Put together the pieces and wala!
You have a nice app running.&lt;/p&gt;
&lt;p&gt;I think that`s all people.&lt;/p&gt;
&lt;p&gt;Feel free to contribute, share your opinion about the design decisions and technologies.&lt;/p&gt;
&lt;p&gt;Thanks!&lt;/p&gt;
&lt;p&gt;&lt;a href="http://github.com/fforbeck/twitter-analyst"&gt;&lt;img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/38ef81f8aca64bb9a64448d0d70f1308ef5341ab/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[]`s,&lt;/p&gt;
&lt;p&gt;Felipe Forbeck.&lt;/p&gt;</summary><category term="play framework"></category><category term="hp-idol"></category><category term="sentiment analysis"></category></entry><entry><title>Engineering culture @ Spotify</title><link href="http://felipeforbeck.com/posts/2014/07/engineering-culture-spotify/" rel="alternate"></link><updated>2014-07-21T22:08:00-07:00</updated><author><name>Felipe Forbeck</name></author><id>tag:felipeforbeck.com,2014-07-21:posts/2014/07/engineering-culture-spotify/</id><summary type="html">&lt;div class="vimeo" align="center"&gt;&lt;iframe width="800" height="500" src="https://player.vimeo.com/video/85490944" frameborder="0"&gt;&lt;/iframe&gt;&lt;/div&gt;</summary><category term="software &amp; product development"></category><category term="spotify"></category></entry><entry><title>Revolution OS</title><link href="http://felipeforbeck.com/posts/2014/07/the-linux-story/" rel="alternate"></link><updated>2014-07-21T21:41:00-07:00</updated><author><name>Felipe Forbeck</name></author><id>tag:felipeforbeck.com,2014-07-21:posts/2014/07/the-linux-story/</id><summary type="html">&lt;ul class="simple"&gt;
&lt;li&gt;How does the Linux OS was created?&lt;/li&gt;
&lt;li&gt;Why the community work is so important and powerful?&lt;/li&gt;
&lt;li&gt;Why do you should care about free and open source software?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Take a look at this documentary, you can have the answer for this and many other questions.&lt;/p&gt;
&lt;div class="youtube" align="center"&gt;&lt;iframe width="800" height="500" src="https://www.youtube.com/embed/plMxWpXhqig" frameborder="0"&gt;&lt;/iframe&gt;&lt;/div&gt;</summary><category term="linux"></category><category term="open source"></category><category term="free software"></category></entry><entry><title>The Internet's Own Boy</title><link href="http://felipeforbeck.com/posts/2014/07/the-internets-own-boy/" rel="alternate"></link><updated>2014-07-21T21:10:00-07:00</updated><author><name>Felipe Forbeck</name></author><id>tag:felipeforbeck.com,2014-07-21:posts/2014/07/the-internets-own-boy/</id><summary type="html">&lt;p&gt;A great documentary about Aaron Swartz, a programmer and information activist. Helped in the development the RSS and was co-founder of Reddit. He fought for free information access and he was ridiculously overwhelmed by the system.&lt;/p&gt;
&lt;p&gt;hope he can inspire many people...&lt;/p&gt;
&lt;div class="youtube" align="center"&gt;&lt;iframe width="800" height="500" src="https://www.youtube.com/embed/ehxGPRgUvzc" frameborder="0"&gt;&lt;/iframe&gt;&lt;/div&gt;</summary><category term="information activism"></category><category term="hacktivism"></category><category term="free sharing"></category></entry></feed>