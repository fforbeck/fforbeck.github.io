<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>a bit of bits...</title><link href="http://felipeforbeck.com/" rel="alternate"></link><link href="http://felipeforbeck.com/feeds/felipe-forbeck.atom.xml" rel="self"></link><id>http://felipeforbeck.com/</id><updated>2016-05-28T17:20:00-07:00</updated><entry><title>Vacuum - REST API backed with Neo4j</title><link href="http://felipeforbeck.com/posts/2016/05/java-rest-api-microservice-dependency-graph/" rel="alternate"></link><updated>2016-05-28T17:20:00-07:00</updated><author><name>Felipe Forbeck</name></author><id>tag:felipeforbeck.com,2016-05-28:posts/2016/05/java-rest-api-microservice-dependency-graph/</id><summary type="html">&lt;h3&gt;Motivation&lt;/h3&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;a href="http://vanhackathon.devpost.com/"&gt;Vanhack&lt;/a&gt; promoted a hackathon for programmers, designers and digital marketers who want to show their skills to Canadian technology companies. &lt;a href="http://vanhackathon.devpost.com/"&gt;Vanhack&lt;/a&gt; is a startup that helps skilled IT people from all around the world to get awesome jobs in Canada. This was the first hackathon they promoted and they already said that we will have more.&lt;/p&gt;
&lt;p&gt;For this edition we had companies such as &lt;a href="https://www.axiomzen.co/"&gt;AxiomZen&lt;/a&gt;, &lt;a href="http://hootsuite.com/"&gt;Hootsuite&lt;/a&gt;, &lt;a href="http://shopify.com"&gt;Shopify&lt;/a&gt; and many other. You can take a look &lt;a href="https://www.vanhack.com/hackathon/"&gt;here&lt;/a&gt;. Each company proposed one or more challenges for back-end, front-end, UX Designers, etc. 
I've developed a REST API in Java with Neo4J for the challenge proposed by &lt;a href="http://hootsuite.com/"&gt;Hootsuite&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I really liked the challenge which you can see the basic description bellow:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Build a system using graph DB to represent the microservice dependency graph.&lt;/li&gt;
&lt;li&gt;Imagine thousands of microservices calling each other on different API endpoints using REST:&lt;ul&gt;
&lt;li&gt;How would you know which service depends on another? &lt;/li&gt;
&lt;li&gt;How often API calls are being made from one service to another? &lt;/li&gt;
&lt;li&gt;Does one service always call all endpoints of another service, or usually just one or two?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It was an open ended challenge.&lt;/p&gt;
&lt;p&gt;I decided to move forward with this challenged because I would like to use a graph database, mainly &lt;a href="http://neo4j.com/"&gt;Neo4J&lt;/a&gt; and learn a bit more. I already did some small apps and proof of concepts with it in the past, but then I never touch it again.&lt;/p&gt;
&lt;h3&gt;Problem&lt;/h3&gt;
&lt;hr /&gt;
&lt;p&gt;The first thing was to decide in which language I would develop the API. I was wondering if I should use Java or Python.&lt;/p&gt;
&lt;p&gt;For Python I could use &lt;a href="http://www.tornadoweb.org/en/stable/"&gt;Tornado&lt;/a&gt;, which is web framework and asynchronous networking library. Pretty easy to install and use.
Python usually I write less code than Java to get the things done and I already created a REST API with Python and Tornado, but never using a graph db. &lt;/p&gt;
&lt;p&gt;For Java I could use &lt;a href="https://playframework.com/"&gt;Play Framework&lt;/a&gt; which is also a web framework built on top of &lt;a href="http://akka.io/"&gt;Akka&lt;/a&gt;. I already created an App using Play Framework and it was very productive and easy to use. You can check it &lt;a href="posts/2015/01/tweet-sentiment-analyst/"&gt;here&lt;/a&gt;. 
Also I could use &lt;a href="http://projects.spring.io/spring-boot/"&gt;Spring Boot&lt;/a&gt; which I've used in the past in some consulting projects and you can enable maven libraries in your project to make it a web project, standalone app and so on.&lt;/p&gt;
&lt;p&gt;Then I started thinking about how Neo4J would communicate with this two languages and, fortunately, it has official drivers for both languages: &lt;a href="http://neo4j.com/developer/java/"&gt;Java&lt;/a&gt; &amp;amp; &lt;a href="http://neo4j.com/developer/python/"&gt;Python&lt;/a&gt;.
After some research I thought it would be good to use Java. Mainly because it has more options, besides the native drivers, to integrate with Neo4J. For instance: Spring-Data-Neo4j, Neo4j's Embedded Java API, JPA and JDBC.&lt;/p&gt;
&lt;p&gt;I thought that I could easily use Spring-Data-Neo4j with Spring-Boot and start the App with a few lines of code, because spring-data provides that &lt;a href="http://docs.spring.io/spring-data/data-commons/docs/1.6.1.RELEASE/reference/html/repositories.html"&gt;magic crud interface for repositories that allows you to easily access your persistence layer&lt;/a&gt;.
So, I just decided to move forward with Java and Spring-Data-Neo4J. Assuming that I could try another approaches to integrate with Neo4j is something else wasn't working as expected.&lt;/p&gt;
&lt;h3&gt;Solution&lt;/h3&gt;
&lt;hr /&gt;
&lt;p&gt;Vacuum is the REST API which allows you to understand the dependency graph of your microservices architecture.&lt;/p&gt;
&lt;p&gt;It is based on &lt;a href="http://swagger.io/"&gt;Swagger.io&lt;/a&gt;, you can just submit the URL of your swagger documentation
and Vacuum will parse it and create the Service-Endpoint graph as you can see in the image bellow.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Service-Endpoint-Graph" src="/images/graph-microservice-dependencies.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nodes&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Service&lt;/code&gt;: is the &lt;em&gt;green&lt;/em&gt; node which represents the service host name. It is captured from Swagger API document. For Neo4J it is called &lt;code&gt;Label&lt;/code&gt;. One node can have multiple lables. You can picture it as a node type. It is important to define the label/type for your nodes because it helps you to perform more interesting queries. The nodes with this type will have two properties: &lt;code&gt;host&lt;/code&gt; and &lt;code&gt;uuid&lt;/code&gt;, in which &lt;em&gt;host&lt;/em&gt; is the name of the host and &lt;em&gt;uuid&lt;/em&gt; is generated for each new service registered in the system.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Endpoint&lt;/code&gt;: is the &lt;em&gt;blue&lt;/em&gt; node which represents the service endpoints. It is also captured from Swagger API document. It is a label and it has the property &lt;code&gt;path&lt;/code&gt;, which is the path that the endpoint is exposing, for instance &lt;code&gt;/v1/users/{id}&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Relationships&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;EXPOSES&lt;/code&gt;: it is a type/label which defines the relationship between two nodes &lt;code&gt;Service&lt;/code&gt; and &lt;code&gt;Endpoint&lt;/code&gt;. It is an edge from &lt;code&gt;Service&lt;/code&gt; that points to &lt;code&gt;Endpoint&lt;/code&gt; node. It is also built based on swagger spec. With this we can see which endpoints each service is providing. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;CALL&lt;/code&gt;: it is a relationship created when you send a request to Vacuum API that reflects the call from service A to B.
This relationship contains a property called &lt;code&gt;count&lt;/code&gt;, every registered call from A to B, &lt;code&gt;count&lt;/code&gt; is incremented.
With that you can keep track of the number of calls from A to B.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;POST&lt;/code&gt;, &lt;code&gt;GET&lt;/code&gt;, &lt;code&gt;DELETE&lt;/code&gt; and other HTTP methods: are directed relationships created between two services A and B or B to A. It depends on
which service is calling. We might have multiple relationships here because we can call the same service in different endpoints with different http methods.
So, if service A calls a endpoint &lt;code&gt;/v1/test&lt;/code&gt; using method &lt;code&gt;GET&lt;/code&gt; Vacuum will increment the &lt;code&gt;CALL&lt;/code&gt; relationship and create a new one from A to B called &lt;code&gt;GET&lt;/code&gt;,
if it already exists we also increment the &lt;code&gt;count&lt;/code&gt;. Vacuum also stores in the relationship the &lt;code&gt;path&lt;/code&gt; that is being called. In this case &lt;code&gt;path&lt;/code&gt; property would be populated
with &lt;code&gt;/v1/test&lt;/code&gt;. &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Examples&lt;/strong&gt; (Node--Relationship--&amp;gt;Node):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Event 1 &lt;strong&gt;A&lt;/strong&gt; calls &lt;code&gt;GET&lt;/code&gt; &lt;em&gt;/v1/test&lt;/em&gt; on &lt;strong&gt;B&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;(Service A)---(GET {path: /v1/test, count:1})---&amp;gt;(Service B)&lt;/li&gt;
&lt;li&gt;(Service A)---(CALL {count:1})---&amp;gt;(Service B)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Event 2 &lt;strong&gt;A&lt;/strong&gt; calls &lt;code&gt;DELETE&lt;/code&gt; &lt;em&gt;/v1/test&lt;/em&gt; on &lt;strong&gt;B&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;(Service A)---(DELETE {path: /v1/test, count:1})---&amp;gt;(Service B)&lt;/li&gt;
&lt;li&gt;(Service A)---(CALL {count:2})---&amp;gt;(Service B)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Registering your service&lt;/h4&gt;
&lt;p&gt;The first step is to register your microservice in the Vacuum API, for that you need to send a post request to &lt;code&gt;/v1/microservices&lt;/code&gt; passing the swagger url of your API.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Creating a Service-Endpoint graph based on a Swagger URL&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;POST &amp;lt;host&amp;gt;:8090/v1/microservices -H &amp;#39;Content-Type: application/json&amp;#39; -d &amp;#39;{&amp;quot;swagger_url&amp;quot;: &amp;quot;&amp;lt;the_swagger_url&amp;gt;&amp;quot;}&amp;#39;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Registering a call from service A to service B&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Once you have registered your service you can fire a POST request to Vacuum API when your microservice A is calling B. The request needs to be populated with the details about the call from A to B.
These are parameters that need to be sent in the request which represents the event call from A to B:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;origin_host:  The host name from service A&lt;/li&gt;
&lt;li&gt;method:       The HTTP method that is being called on service B&lt;/li&gt;
&lt;li&gt;target_host:  The host name from service B&lt;/li&gt;
&lt;li&gt;target_path:  The endpoint that service A is calling on B&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;POST &amp;lt;host&amp;gt;:8090/v1/requests -H &amp;#39;Content-Type: application/json&amp;#39; -d &amp;#39;{&amp;quot;origin_host&amp;quot;: &amp;quot;api.uber.com&amp;quot;, &amp;quot;method&amp;quot;: &amp;quot;POST&amp;quot;, &amp;quot;target_host&amp;quot;: &amp;quot;petstore.swagger.io&amp;quot;, &amp;quot;target_path&amp;quot;: &amp;quot;/v2/user/login&amp;quot;}&amp;#39;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;After that, Vacuum will create the relationships between both services considering the information you have provided in the request.
Both services need to be registered in the Vacuum API.&lt;/p&gt;
&lt;h4&gt;Basic Queries&lt;/h4&gt;
&lt;p&gt;These are the basic queries that I have implemented that you can use to get more information about the calls that your services are executing. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Get all microservice which contains term &lt;code&gt;user&lt;/code&gt; in ther endpoints&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;GET &amp;lt;host&amp;gt;:8090/v1/microservices?path_term=user -H &amp;#39;Content-Type: application/json&amp;#39;

@Override
public List&amp;lt;Microservice&amp;gt; findMicroservicesByPathTerm(String term) {
    List&amp;lt;Microservice&amp;gt; microservices = new ArrayList&amp;lt;&amp;gt;();
    Driver driver = connector.getDriver();
    HashMap&amp;lt;String, Object&amp;gt; params = new HashMap&amp;lt;&amp;gt;();
    params.put(&amp;quot;term&amp;quot;, term);
    try (Session session = driver.session();
         Transaction tx = session.beginTransaction()) {
        StatementResult r = tx.run(&amp;quot;MATCH (s:Service)-[:EXPOSES]-&amp;gt;(e:Endpoint)
                WHERE e.path CONTAINS {term} RETURN DISTINCT s.host&amp;quot;,
                params);
        r.forEachRemaining(record -&amp;gt; microservices.add(new Microservice(record.get(&amp;quot;s.host&amp;quot;).asString())));
        tx.success();
    }
    return microservices;
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Get all services which rely on service id&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;GET &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;host&lt;span class="o"&gt;&amp;gt;:&lt;/span&gt;&lt;span class="m"&gt;8090&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;v1&lt;span class="o"&gt;/&lt;/span&gt;microservices&lt;span class="o"&gt;/&lt;/span&gt;b36e8649&lt;span class="o"&gt;-&lt;/span&gt;e82e&lt;span class="m"&gt;-4795-8&lt;/span&gt;ef6&lt;span class="o"&gt;-&lt;/span&gt;c2d8eb3e6620&lt;span class="o"&gt;/&lt;/span&gt;dependants &lt;span class="o"&gt;-&lt;/span&gt;H &lt;span class="s"&gt;&amp;#39;Content-Type: application/json&amp;#39;&lt;/span&gt;

&lt;span class="o"&gt;@&lt;/span&gt;Override
public List&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;Microservice&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; findDependants&lt;span class="p"&gt;(&lt;/span&gt;String microserviceId&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    List&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;Microservice&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; microservices &lt;span class="o"&gt;=&lt;/span&gt; new ArrayList&lt;span class="o"&gt;&amp;lt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;

    HashMap&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;String&lt;span class="p"&gt;,&lt;/span&gt; Object&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; params &lt;span class="o"&gt;=&lt;/span&gt; new HashMap&lt;span class="o"&gt;&amp;lt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
    params.put&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;uuid&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; microserviceId&lt;span class="p"&gt;);&lt;/span&gt;

    Driver driver &lt;span class="o"&gt;=&lt;/span&gt; connector.getDriver&lt;span class="p"&gt;();&lt;/span&gt;
    try &lt;span class="p"&gt;(&lt;/span&gt;Session session &lt;span class="o"&gt;=&lt;/span&gt; driver.session&lt;span class="p"&gt;();&lt;/span&gt;
         Transaction tx &lt;span class="o"&gt;=&lt;/span&gt; session.beginTransaction&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        StringBuilder sb &lt;span class="o"&gt;=&lt;/span&gt; new StringBuilder&lt;span class="p"&gt;();&lt;/span&gt;
        sb.append&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;MATCH (s1:Service {uuid: {uuid}})&amp;lt;-[:CALL]-(s2:Service)&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        sb.append&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot; RETURN s2.host&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        StatementResult r &lt;span class="o"&gt;=&lt;/span&gt; tx.run&lt;span class="p"&gt;(&lt;/span&gt;sb.toString&lt;span class="p"&gt;(),&lt;/span&gt; params&lt;span class="p"&gt;);&lt;/span&gt;
        r.forEachRemaining&lt;span class="p"&gt;(&lt;/span&gt;record &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; microservices.add&lt;span class="p"&gt;(&lt;/span&gt;new Microservice&lt;span class="p"&gt;(&lt;/span&gt;record.get&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;s2.host&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="m"&gt;.&lt;/span&gt;asString&lt;span class="p"&gt;())));&lt;/span&gt;
        tx.success&lt;span class="p"&gt;();&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="kr"&gt;return&lt;/span&gt; microservices&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The microserviceId is generate and returned when you send a &lt;code&gt;POST /v1/microservices&lt;/code&gt; with &lt;code&gt;swagger_url&lt;/code&gt; as body param. Or you can just checkout the &lt;code&gt;uuid&lt;/code&gt; param on Neo4j dashboard if you have executed the &lt;code&gt;fetch_db.sh&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;Final Thoughts&lt;/h3&gt;
&lt;hr /&gt;
&lt;p&gt;It is important to mention that all queries can be easily add using the &lt;a href="http://neo4j.com/docs/developer-manual/current/#cypher-query-lang"&gt;Cypher query language&lt;/a&gt;.
&lt;em&gt;"Cypher is a declarative graph query language that allows for expressive and efficient querying and updating of the graph store."&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In addition, I saw spring-data-neo4j allows you to create multiple labels for the same node if you create a parent class. So, all parent classes are added as labels for a node. It is important, because adding more labels to a node means that you can represent more information and ask different questions. &lt;/p&gt;
&lt;p&gt;I did not planned to use multiple labels for the same node, but if I had to, I did not want to build a inheritance structure just to represent that. I know it is the logical and OO alternative, but it is also more code to test and maintain. So I decided to try the &lt;a href="http://neo4j.com/developer/java/#neo4j-java-driver"&gt;Java Driver for Neo4J&lt;/a&gt; instead.&lt;/p&gt;
&lt;p&gt;Another point about using the Java Native Driver is that you can write your own traversal algorithms for the graph. This might be a good idea when you have a more complex graph and you really know some shortcuts in the model to get faster results than using cypher queries. At the moment, &lt;code&gt;Service&lt;/code&gt;-&lt;code&gt;Endpoint&lt;/code&gt; graph is pretty simple, though. Cypher query can solve all my problems, but if I need something more elaborated I can easily implement a new Repository with custom traversal routines.&lt;/p&gt;
&lt;p&gt;Given that, I had to create a connector which starts the connection with my local Neo4J instance and tests if it is possible to open a new session.
Also, it is important to close the connection when your app is terminated, so I added the call on &lt;code&gt;preDestroy&lt;/code&gt; method.&lt;/p&gt;
&lt;p&gt;The connector can be a singleton and can be shared between your services because you need to start a new session whenever you need to send a command to Neo4J. You also need to close the session after the work is done. The session creation is thread safe and you can see the sample connector bellow:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.log4j.Logger&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.neo4j.driver.v1.Driver&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.neo4j.driver.v1.GraphDatabase&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.neo4j.driver.v1.Session&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.springframework.stereotype.Component&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;javax.annotation.PostConstruct&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;javax.annotation.PreDestroy&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="nd"&gt;@Component&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Neo4JConnector&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;

    &lt;span class="n"&gt;private&lt;/span&gt; &lt;span class="n"&gt;static&lt;/span&gt; &lt;span class="n"&gt;final&lt;/span&gt; &lt;span class="n"&gt;Logger&lt;/span&gt; &lt;span class="n"&gt;LOG&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Logger&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getLogger&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Neo4JConnector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;class&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

    &lt;span class="n"&gt;private&lt;/span&gt; &lt;span class="n"&gt;Driver&lt;/span&gt; &lt;span class="n"&gt;driver&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

    &lt;span class="nd"&gt;@PostConstruct&lt;/span&gt;
    &lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;void&lt;/span&gt; &lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;this&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;driver&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GraphDatabase&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;driver&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;bolt://localhost&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="n"&gt;ping&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
        &lt;span class="n"&gt;LOG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Graph DB started&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="n"&gt;private&lt;/span&gt; &lt;span class="n"&gt;void&lt;/span&gt; &lt;span class="n"&gt;ping&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;Session&lt;/span&gt; &lt;span class="n"&gt;session&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;this&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;driver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;session&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
        &lt;span class="n"&gt;session&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isOpen&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
        &lt;span class="n"&gt;session&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="nd"&gt;@PreDestroy&lt;/span&gt;
    &lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;void&lt;/span&gt; &lt;span class="n"&gt;terminate&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;driver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
        &lt;span class="n"&gt;LOG&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Graph DB terminated&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;Driver&lt;/span&gt; &lt;span class="n"&gt;getDriver&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;driver&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Well, I have built the pretty basic structure for the Vacuum API. We are able to register and extract some information about the dependency graph of our services architecture, see the graph on Neo4J dashboard and easily add new queries. &lt;/p&gt;
&lt;p&gt;However, I know there is a ton of work to be done yet and many things can be improved. I am happy with the solution so far and I wish I had more time to implement other queries before the hackathon deadline. The main I idea is in place. I am planning to continue with it as a side project.&lt;/p&gt;
&lt;h3&gt;Future work&lt;/h3&gt;
&lt;hr /&gt;
&lt;ul&gt;
&lt;li&gt;Add more queries&lt;/li&gt;
&lt;li&gt;Document Vacuum API with Swagger.io&lt;/li&gt;
&lt;li&gt;Add authentication for Vacuum API,&lt;/li&gt;
&lt;li&gt;Enable Neo4J authentication and implement it on Vacuum API&lt;/li&gt;
&lt;li&gt;Create the Unit &amp;amp; IT Tests (Neo4J provides a test db for your IT tests)&lt;/li&gt;
&lt;li&gt;Improve DDD&lt;/li&gt;
&lt;li&gt;Add Java DOCs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://github.com/fforbeck/vacuum"&gt;&lt;img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/38ef81f8aca64bb9a64448d0d70f1308ef5341ab/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"&gt;&lt;/a&gt;&lt;/p&gt;</summary><category term="open-source"></category><category term="rest"></category><category term="hackathon"></category><category term="neo4j"></category><category term="java"></category><category term="api"></category><category term="spring-boot"></category><category term="swagger"></category></entry><entry><title>The Developers Conference, Floripa 2016 - Best Talks</title><link href="http://felipeforbeck.com/posts/2016/05/best-talks-tdc-2016-floripa/" rel="alternate"></link><updated>2016-05-14T19:20:00-07:00</updated><author><name>Felipe Forbeck</name></author><id>tag:felipeforbeck.com,2016-05-14:posts/2016/05/best-talks-tdc-2016-floripa/</id><summary type="html">&lt;p&gt;The Developers Conference happens every year in Brazil. This time I've watched online and created this post with the best talks in different subjects. They are all in Portuguese.&lt;/p&gt;
&lt;h3&gt;&lt;a href="http://bit.ly/1TxvN3O"&gt;2016: Uma Odisséia no Cassandra&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Paulo Motta&lt;/code&gt; - integrante do time de desenvolvimento do Apache Cassandra, fala sobre as principais funcionalidades e aplicações do NoSQL Canssandra.
Comenta casos de uso de empresas como Netflix, Instagram, Spotify, etc. Apresenta exemplos de arquitetura e modelagem utilizados por tais empresas e fala Materialized Views.
- &lt;a href="http://bit.ly/1TxvN3O"&gt;http://bit.ly/1TxvN3O&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a href="http://bit.ly/275ZKlD"&gt;Crowdfunding para projetos de IoT: o que aprendemos com a IoT Surfboard&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Vinicius Senger&lt;/code&gt; - Apresenta os resultados de uma experiência de financiamento coletivo da IoT Surfboard com 210% da meta atingida no Catarse. Comenta sobre a sua experiência e aprendizado na área de IoT e da dicas para quem pensa em criar um produto e não tem dinheiro para investir na produção.
- &lt;a href="http://bit.ly/275ZKlD"&gt;http://bit.ly/275ZKlD&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a href="http://bit.ly/1Xg0ro2"&gt;Cloud Foundry a Plataforma Digital&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Victor Fonseca&lt;/code&gt; -  Apresenta Spring Cloud mostrando como é prático criar microserviços na arquitetura corporativa. Apresenta o Cloud Foundry que é um PaaS open source criado pela VMWare e agora mantido pela Pivotal Software.
- &lt;a href="http://bit.ly/1Xg0ro2"&gt;http://bit.ly/1Xg0ro2&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a href="http://bit.ly/1Txt5LN"&gt;Vantagens e desvantagens de trabalhar remoto&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Rodrigo Urubatan Ferreira Jardim&lt;/code&gt; -  Comenta sobre as vantagens e desvantagens de trabalhar remoto. Descreve sua experiência e fornece dicas para quem tem interesse em se aventurar nesse modo de trabalho que está cada vez mais comum.
- &lt;a href="http://bit.ly/1Txt5LN"&gt;http://bit.ly/1Txt5LN&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a href="http://bit.ly/1s5VDWR"&gt;Android Studio 2.0 What's new?&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Thiago Nunes Cechetto&lt;/code&gt; - Apresenta novidades do novo Android Studio, considerando a versão mais recente, em qual houve a melhora da velocidade de instalação e execução dos aplicativos em desenvolvimento. Além disso demonstra funcionalidades interessantes relacionadas ao novo emulador.
- &lt;a href="http://bit.ly/1s5VDWR"&gt;http://bit.ly/1s5VDWR&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a href="http://bit.ly/1Xg32hO"&gt;Aprendendo Docker sem bruxaria&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Stefan Teixeira&lt;/code&gt; - Apresenta conceitos fundamentais sobre Docker, conteinerização. Cria uma aplicação web + banco de dados utilizando containers, testa a aplicação manualmente e demonstra testes de API automatizados.
- &lt;a href="http://bit.ly/1Xg32hO"&gt;http://bit.ly/1Xg32hO&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a href="http://bit.ly/1TxtaPF"&gt;Node.js - Levando o poder do JavaScript para o servidor!&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Rodrigo Branas&lt;/code&gt; - Aprensenta conceitos fundamentais sobre Node.js, arquitetura assíncrona e não bloqueante e principais módulos. Em um quick hands-on, uma API é criada para atender as requisições de um aplicativo escrito em AngularJS.
- &lt;a href="http://bit.ly/1TxtaPF"&gt;http://bit.ly/1TxtaPF&lt;/a&gt;&lt;/p&gt;</summary><category term="tdc-2016"></category><category term="conference"></category><category term="talks"></category></entry><entry><title>Videos about Quantum Computing + Free access to online IBM Quantum Computer</title><link href="http://felipeforbeck.com/posts/2016/05/quantum-computing-concepts-and-ibm-qc-free-access/" rel="alternate"></link><updated>2016-05-12T09:20:00-07:00</updated><author><name>Felipe Forbeck</name></author><id>tag:felipeforbeck.com,2016-05-12:posts/2016/05/quantum-computing-concepts-and-ibm-qc-free-access/</id><summary type="html">&lt;p&gt;Old but gold. &lt;/p&gt;
&lt;h4&gt;Quantum physicist Michelle Simmons @ TEDxSydney 2012 gives the best explanation about Quantum Computing I've watched&lt;/h4&gt;
&lt;div class="youtube" align="center"&gt;
    &lt;iframe width="800" height="500" src="http://youtube.com/embed/cugu4iW4W54" frameborder="0"&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h4&gt;If you prefer a short version by the theoretical physicist Lawrence Krauss&lt;/h4&gt;
&lt;div class="youtube" align="center"&gt;
    &lt;iframe width="800" height="500" src="http://youtube.com/embed/UUpqnBzBMEE" frameborder="0"&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h4&gt;IBM has release an online service which allows you to play with quantum computer&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;You can request access here:&lt;/code&gt; &lt;a href="https://quantumexperience.ng.bluemix.net/"&gt;https://quantumexperience.ng.bluemix.net/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You can create quantum algorithms, test and evaluate IBM’s Quantum Computer. It is for research purposes only and &lt;em&gt;results are not guaranteed&lt;/em&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;IBM Quantum Experience&lt;/code&gt;, consists of access to an actual Quantum Processor, Quantum Composer, Quantum Simulator, Scores, Results and Quantum website.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Scores&lt;/code&gt;, are user algorithms created by dragging gates to control Qubit functions in the Quantum Composer GUI to create Results.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Quantum Users Results&lt;/code&gt;, are results from running your score on the Quantum Hardware or Quantum Simulator.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Once you get the access, they also provide a good material to teach you the basics about quantum computing.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Here is a demo about the IBM Quantum Experience&lt;/h4&gt;
&lt;div class="youtube" align="center"&gt;
    &lt;iframe width="800" height="500" src="http://youtube.com/embed/pYD6bvKLI_c" frameborder="0"&gt;&lt;/iframe&gt;
&lt;/div&gt;</summary><category term="quantum-computing"></category><category term="ibm-bluemix"></category><category term="concepts"></category></entry><entry><title>Elasticsearch - Server and Client config notes</title><link href="http://felipeforbeck.com/posts/2016/05/elasticsearch-notes-about-server-client-configs/" rel="alternate"></link><updated>2016-05-11T20:36:00-07:00</updated><author><name>Felipe Forbeck</name></author><id>tag:felipeforbeck.com,2016-05-11:posts/2016/05/elasticsearch-notes-about-server-client-configs/</id><summary type="html">&lt;h3&gt;ES Server - Elasticsearch v2.0.0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;cluster.routing.allocation.cluster_concurrent_rebalance&lt;/code&gt;: 2&lt;ul&gt;
&lt;li&gt;Determines the number of shards allowed for concurrent rebalance. This property needs to be set appropriately depending on the hardware being used, for example the number of CPUs, IO capacity, etc. If this property is not set appropriately, it can impact the ElasticSearch performance with indexing. By default the value is set at 2, meaning that at any point in time only 2 shards are allowed to be moving. It is good to set this property low so that the rebalance of shards is throttled and doesn't affect indexing.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;index.store.throttle.max_bytes_per_sec&lt;/code&gt;: 10mb&lt;ul&gt;
&lt;li&gt;Allows to control the maximum bytes per sec written to the file system. I was using small documents, something around 10Kb, we can increase this value once we get bigger documents.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;index.number_of_replicas&lt;/code&gt;: 0&lt;ul&gt;
&lt;li&gt;To disable the replica allocation in order to run a single node per cluster. &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;index.routing.allocation.total_shards_per_node&lt;/code&gt;: 2&lt;ul&gt;
&lt;li&gt;The maximum number of shards (replicas and primaries) that will be allocated to a single node. Defaults is unbounded. It imposes a hard limit which can result in some shards not being allocated. Use with caution. It can be changed later to add more shards and help the search feature, because replica shards will receive the queries while primary shards will receive the index and delete requests.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;index.refresh_interval&lt;/code&gt;: 5s&lt;ul&gt;
&lt;li&gt;Better indexing performance if you leave refresh enabled. This is because ES a separate refresh thread which will do the flushing, instead of having your bulk indexing threads to it when RAM is full.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;indices.cluster.send_refresh_mapping&lt;/code&gt;: false&lt;ul&gt;
&lt;li&gt;When the index manager send a node an index request to process, the node updates its own mapping and then sends that mapping to the master. While the master processes it, that node receives a state that includes an older version of the mapping. If there’s a conflict, it’s not bad (i.e. the cluster state will eventually have the correct mapping), but we send a refresh just in case from that node to the master. In order to make the index request more efficient, we have set this property on our data nodes. We are currently running one single node so we do not need to have it enabled.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;node.max_local_storage_nodes&lt;/code&gt;: 1&lt;ul&gt;
&lt;li&gt;Start at most one single node in the cluster.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;action.destructive_requires_name&lt;/code&gt;: true&lt;ul&gt;
&lt;li&gt;The delete index API can also be applied to more than one index, or on all indices by using _all or * as index. To prevent deleting all indices via wildcards or _all.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;ES Java Client - Elasticsearch v2.0.0&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;TransportClient&lt;/code&gt; instead of &lt;code&gt;NodeClient&lt;/code&gt;&lt;ul&gt;
&lt;li&gt;Connects to the cluster and does not act like a new node, which reduce the noise in the cluster and allow faster requests.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Parallel bulk requests and do not wait for ES Responses&lt;ul&gt;
&lt;li&gt;ES execute the bulk requests in background and it takes time to send back a response, so just send more bulk requests and put the listeners to parse the responses in another thread.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;BulkProcessorListener&lt;/code&gt; instead of &lt;code&gt;client.prepareBulk()&lt;/code&gt;&lt;ul&gt;
&lt;li&gt;The listener allow you to configure the size of the bulk request and other flush parameters as well.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;BulkProcessorListener&lt;/code&gt; actions: 1000-10000 (current is 5K)&lt;ul&gt;
&lt;li&gt;Do not set more than 10K per bulk actions, it is not recommended.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;BulkProcessorListener&lt;/code&gt; with BulkSize in MB&lt;ul&gt;
&lt;li&gt;Set auto flush to the buffer when it reaches &lt;code&gt;X&lt;/code&gt; MB, even if it contains less than &lt;code&gt;N&lt;/code&gt; actions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;BulkProcessorListener&lt;/code&gt; with concurrentRequests enabled (x &amp;gt;= 1)&lt;ul&gt;
&lt;li&gt;To avoid blocking threads for bulk requests is recommended to set this property using &lt;code&gt;x = 4 * num_available_cores&lt;/code&gt; for the concurrency.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary><category term="elasticsearch"></category><category term="configs"></category><category term="java"></category></entry><entry><title>LOL - who needs help?</title><link href="http://felipeforbeck.com/posts/2016/05/lol-who-needs-help/" rel="alternate"></link><updated>2016-05-10T20:35:00-07:00</updated><author><name>Felipe Forbeck</name></author><id>tag:felipeforbeck.com,2016-05-10:posts/2016/05/lol-who-needs-help/</id><summary type="html">&lt;p&gt;&lt;img alt="Results" src="/images/crazy-dev.gif" /&gt;&lt;/p&gt;</summary><category term="programmer"></category><category term="gif"></category><category term="lol"></category></entry><entry><title>Free CDN Provider - Couldflare</title><link href="http://felipeforbeck.com/posts/2016/05/free-cdn-provider-cloudflare/" rel="alternate"></link><updated>2016-05-09T20:35:00-07:00</updated><author><name>Felipe Forbeck</name></author><id>tag:felipeforbeck.com,2016-05-09:posts/2016/05/free-cdn-provider-cloudflare/</id><summary type="html">&lt;h3&gt;CDN&lt;/h3&gt;
&lt;hr /&gt;
&lt;p&gt;A content delivery network (CDN) is a distributed network of proxy servers deployed in multiple data centers around the world.
Using a CDN service to serve static content will make your website/blog faster. Also it will give a better user experience to your users.
With a CDN you can serve text, graphics, scripts, media files, documents, etc. 
There are several providers out there, but one that I particularly liked because its easy to use, configure and free, is called &lt;a href="https://www.cloudflare.com/"&gt;CloudFlare&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I've configure this blog to use CloudFlare and it is pretty simple. In the video bellow you can see the main feature that you can use if you subscribe for a free plan.&lt;/p&gt;
&lt;div class="vimeo" align="center"&gt;
    &lt;iframe style="width:100%;min-height=480px;" src="https://player.vimeo.com/video/153411340" frameborder="0"&gt;&lt;/iframe&gt;
&lt;/div&gt;</summary><category term="cloudflare"></category><category term="tool"></category><category term="free"></category></entry><entry><title>HP Haven Twitter Analysis Tutorial Challenge - Results</title><link href="http://felipeforbeck.com/posts/2015/02/topcoder-challenge-tweet-analysis/" rel="alternate"></link><updated>2015-02-14T08:22:00-08:00</updated><author><name>Felipe Forbeck</name></author><id>tag:felipeforbeck.com,2015-02-14:posts/2015/02/topcoder-challenge-tweet-analysis/</id><summary type="html">&lt;p&gt;\o/ I got first place with my &lt;a href="https://github.com/fforbeck/twitter-analyst"&gt;App&lt;/a&gt; :D&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.topcoder.com/challenge-details/30048480/#winner"&gt;Results&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Results" src="/images/hp-haven-twitter-analysis-tutorial-challenge-results.png" /&gt;&lt;/p&gt;</summary><category term="play framework"></category><category term="hp-idol"></category><category term="sentiment analysis"></category><category term="topcoder"></category><category term="chanllenge"></category></entry><entry><title>Tweet Sentiment Analysis</title><link href="http://felipeforbeck.com/posts/2015/01/tweet-sentiment-analyst/" rel="alternate"></link><updated>2015-01-05T21:31:00-08:00</updated><author><name>Felipe Forbeck</name></author><id>tag:felipeforbeck.com,2015-01-05:posts/2015/01/tweet-sentiment-analyst/</id><summary type="html">&lt;h1&gt;Why&lt;/h1&gt;
&lt;p&gt;Some days ago I enrolled in my first topcoder development challenge and it is basically a tutorial challenge where we need to install HPVertica DB, capture a set of tweets and based on $HPQ tag, use the HP Idol platform, which has an API to perform sentiment analysis over a tweet text content.&lt;/p&gt;
&lt;p&gt;With that in mind I decided to start this challenge with Play Framework to learn a little bit and see how
fast I could create this WebApp from scratch without know the tool. Beside that, I have read about play framework and it seems to be a very powerful tool which I could easily plug different technologies and keep focused in my development. It provides you a sort of minimum viable architecture for your system. In addition, another reason to pick this technology is because I can use Java, which was one of the requirements of the challenge.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;The challenge&lt;/code&gt; - &lt;a href="https://www.topcoder.com/challenge-details/30048480/"&gt;https://www.topcoder.com/challenge-details/30048480/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;"Here are the steps to participate in the HP Haven Twitter Analysis Tutorial challenge:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;You’ll be creating a Java application which accesses Twitter data for the Hewlett Packard stock symbol tag “$HPQ”, performs sentiment analysis on this data, and loads the raw social feed and sentiment data into the Vertica database.  The application should also display some kind of visualization about how sentiment is changing over time or by topic.  The application should extract enough Tweets that the Sentiment Analysis shows some depth/variation – at least 1000 Tweets, but more would even be better.&lt;/li&gt;
&lt;li&gt;You have creative license about what kind of application to create.  You may create a mobile, web, or desktop app.&lt;/li&gt;
&lt;li&gt;Your application should connect to the IDOL OnDemand platform to perform the Sentiment Analysis on the Twitter data related to the tag $HPQ.  The Sentiment Analysis results should be stored in your locally configured version of Vertica.  Sample Java code to connect to IDOL OnDemand is attached to the challenge.  Sample code can also be found on the IDOL OnDemand Community site.&lt;/li&gt;
&lt;li&gt;This is a tutorial challenge. Your code should be clear and well documented.&lt;/li&gt;
&lt;li&gt;You should produce a blog post about your application.&lt;/li&gt;
&lt;li&gt;You should produce a screensharing video which explains your code and how to set up and connect to a Vertica database.&lt;/li&gt;
&lt;li&gt;There should be some kind of visualization in your app which displays the Sentiment Scores related to a topic and/or time dimension.&lt;/li&gt;
&lt;li&gt;We're currently running a Sweepstakes challenge which walks through the Vertica setup on a local VMWare instance.  We're also attaching a Vertica lab manual which describes how to add users, create schemas, and load data into the system.  It assumes, however, that you have the Vertica Virtual Server instance installed and locally available."&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;h1&gt;How&lt;/h1&gt;
&lt;h3&gt;Tools and Frameworks&lt;/h3&gt;
&lt;p&gt;Java; &lt;a href="https://www.playframework.com/"&gt;Play Framework&lt;/a&gt;; &lt;a href="https://spring.io/"&gt;Spring&lt;/a&gt;; &lt;a href="http://projects.spring.io/spring-data/"&gt;Spring Data&lt;/a&gt;; &lt;a href="http://hibernate.org/orm/"&gt;Hibernate&lt;/a&gt;; &lt;a href="https://www.playframework.com/documentation/2.3.x/JavaWebSockets"&gt;WebSockets&lt;/a&gt;; &lt;a href="http://akka.io/"&gt;Akka&lt;/a&gt;; &lt;a href="https://dev.twitter.com"&gt;Twitter4J Search &amp;amp; Stream API&lt;/a&gt;; &lt;a href="http://redis.io/"&gt;Redis&lt;/a&gt;; &lt;a href="http://www.vertica.com/about/"&gt;HPVerticaDB&lt;/a&gt;; &lt;a href="https://www.idolondemand.com/developer/apis"&gt;HPIdol OnDemand API&lt;/a&gt;; &lt;a href="http://getbootstrap.com/"&gt;Twitter Bootstrap 3&lt;/a&gt;; &lt;a href="http://jquery.com/"&gt;JQuery 1.11&lt;/a&gt;; &lt;a href="http://www.highcharts.com/"&gt;Highcharts&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Application design&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Tweet Analyst Design" src="/images/tweet_analyst_system_design.png" /&gt;&lt;/p&gt;
&lt;h3&gt;Screencast&lt;/h3&gt;
&lt;div class="vimeo" align="center"&gt;
    &lt;iframe width="800" height="500" src="https://player.vimeo.com/video/119002875" frameborder="0"&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h3&gt;Design notes&lt;/h3&gt;
&lt;h4&gt;&lt;a href="https://dev.twitter.com"&gt;Twitter API&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;Stream&lt;/code&gt; - &lt;a href="https://dev.twitter.com/streaming/overview"&gt;https://dev.twitter.com/streaming/overview&lt;/a&gt;:
"The set of streaming APIs offered by Twitter give developers low latency access to Twitter’s global stream of Tweet data. A proper implementation of a streaming client will be pushed messages indicating Tweets and other events have occurred, without any of the overhead associated with polling a REST endpoint."
In this case I am using the public stream API, it is enough to receive the tweets in real time.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Search&lt;/code&gt; - &lt;a href="https://dev.twitter.com/rest/public/search"&gt;https://dev.twitter.com/rest/public/search&lt;/a&gt;:
"The Twitter Search API is part of Twitter’s v1.1 REST API. It allows queries against the indices of recent or popular Tweets.."
Here I am just using a &lt;code&gt;GET&lt;/code&gt; method for &lt;code&gt;twitter.com/search?q=%MY_HASH_TAG%&lt;/code&gt; URL to search and grab the tweets with the hashTag param.&lt;/p&gt;
&lt;hr /&gt;
&lt;h4&gt;Twitter4J - &lt;a href="http://twitter4j.org/en/index.html"&gt;http://twitter4j.org/en/index.html&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;It is an unofficial library to connect to twitter APIs. You can check the code examples &lt;a href="http://twitter4j.org/en/code-examples.html"&gt;here&lt;/a&gt; and &lt;a href="https://github.com/yusuke/twitter4j/tree/master/twitter4j-examples/src/main/java/twitter4j/examples"&gt;more code examples&lt;/a&gt;
To connect to API you will need some oauth keys and consumer keys.&lt;/p&gt;
&lt;hr /&gt;
&lt;h4&gt;HPIdol OnDemand API - &lt;a href="https://www.idolondemand.com/"&gt;https://www.idolondemand.com/&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;Setiment Analysis API&lt;/code&gt; - &lt;a href="http://www.idolondemand.com/developer/apis/analyzesentiment#overview"&gt;http://www.idolondemand.com/developer/apis/analyzesentiment#overview&lt;/a&gt;
"The Sentiment Analysis API analyzes text to return the sentiment as positive, negative or neutral. It contains a dictionary of positive and negative words of different types, and defines patterns that describe how to combine these words to form positive and negative phrases."
The hello world in java for HP Idol API can be found &lt;a href="https://www.youtube.com/watch?v=MY-ASbxRJVw"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;h4&gt;Redis - &lt;a href="http://redis.io/"&gt;http://redis.io/&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;Processing Queue&lt;/code&gt;:
This queue keeps all the tweets that are in processing phase. For instance, when we receive a new tweet from Twitter Stream API or when we found a set of tweets in Twitter Search API, they are placed in this queue to be processed later.
Processing action here means: we need to parse the tweets, extract the relevant information and send it to HP IDOL API to do the analysis for each one of them. The result of this phase will be placed in another queue (persistent-queue).
'The tweets are saved as String in redis queue.'&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Persistent Queue&lt;/code&gt;:
This queue keeps all the tweets that were processed, analyzed and now are ready to be stored in VerticaDB with
sentiment score.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Live Tweets Channel&lt;/code&gt;:
This channel publishes all the tweets that were processed and analyzed to the subscribers of this channel. It is just a Redis pubSub system, you can learn more about it &lt;a href="http://redis.io/topics/pubsub"&gt;here&lt;/a&gt;.
In this app it is really useful, cause I would like to see the sentiment analysis in real time for all tweets that comes from Twitter Stream API. So, right after analyze it, I just send the results to this channel which will automatically publish this content to every subscriber. The subscriber in this case is an Akka Actor Reference which was created by a websocket connection request, that happens when you open the open the index page with the live tweets chart.&lt;/p&gt;
&lt;hr /&gt;
&lt;h4&gt;&lt;code&gt;HP Vertica DB&lt;/code&gt; - &lt;a href="https://my.vertica.com/community/"&gt;https://my.vertica.com/community/&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;You can download the HP Vertica community edition &lt;a href="https://my.vertica.com/community/"&gt;here&lt;/a&gt;
It is a relational database optimized for large-scale analytics. "It is uniquely designed using a memory-and-disk balanced distributed compressed columnar paradigm, which makes it exponentially faster than older techniques for modern data analytics workloads. HP Vertica supports a series of built-in analytics libraries like time series and analytics packs for geospatial and sentiment plus additional functions from vendors like SAS. And, it supports analytics written using the R programming language for predictive modeling."
More details about the technology can be found &lt;a href="http://www.vertica.com/wp-content/uploads/2014/05/VerticaOverview.pdf"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;h4&gt;Akka Actor System&lt;/h4&gt;
&lt;p&gt;Akka is a framework which provides the set of right tools to build high-scalable and fault-tolerant systems using a Actor System model. You can easily write parallel, concurrent, event-driven programs. To learn about Akka &lt;a href="http://doc.akka.io/docs/akka/2.1.2/intro/what-is-akka.html"&gt;check it out&lt;/a&gt;.
In this project I decided to use Akka to create microservices that would connect with external API`s. In this case my 4 actors play different roles and they are managed by one supervisor.
The idea here is to have a service manage that would orchestrate the messages and actions in my actor system.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Tweet Receiver&lt;/code&gt;:
An Actor Reference which connects to Twitter Stream API using Twitter4J library and listen to tweets from API.
For each new tweet, the actor parses the content and push the result into redis processing-queue.
After that it sends a 'Read' message to the TweetSupervisor.
Currently I am starting only one instance of this actor for the tag+language that I want to receive the tweets.
Later I can easily change to create one instance per tag or something like it.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Tweet Harvester&lt;/code&gt;:
An Actor Reference which connects to Twitter Search API to perform the tweet search based on the tag+language.
It can easily reach the Twitter requests rate limit, so I place this Actor to run every 15 min whatever happens
with it. This setup can be found in TweetSupervisor that we will cover in the next sections. After parse the tweets found
it sends a 'Read' message to TweetSupervisor.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Tweet Analyzer&lt;/code&gt;:
Another Actor Reference which receives a 'Read' message from TweetSupervisor to start reading tweets from processing
queue to send it to HP Idol Sentiment Analysis API. The requests to HP API are synchronous and each result is parsed, appended to the original tweet, sent to the persistent-queue and published to the live-tweets-channel.
N instances of this actor will be created be the supervisor, after each instance execute the job, it finalizes itself.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Tweet Publisher&lt;/code&gt;:
It s an special Actor Reference which is not managed by TweetSupervisor. I`m using this Actor to
handle the tweets published in live-tweets-channel and send it directly to the client.
Cause in the client side we have a scatter chart which shows in real time the tweet sentiment analysis.
For this, was necessary to open a websocket connection per client, each connection is handled by an Actor Reference (Tweet Publisher) that subscribes the redis channel.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Tweet Supervisor&lt;/code&gt;:
It handles two type of messages, Read and Start. This messages are java objects and when an Actor Reference (instance) receives a message that it can handle, it simply execute some action based on it.
So, when the TweetSupervisor receives a Start message it starts two actor instances; TweetReceiver and TweetHarvester.
If the supervisor receives a Read message, it starts a new instance of TweetAnalyzer actor.&lt;/p&gt;
&lt;hr /&gt;
&lt;h4&gt;Service, Repository &amp;amp; Controller&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;Tweet Service&lt;/code&gt;:
Starts the Akka Actor System with the TweetSupervisor, Runs a scheduled job in order to persist
the analyzed tweets, provide some methods to find tweets in TweetRepository.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Tweet Repository&lt;/code&gt;:
It is a spring data repository which provides a set of actions to be done in the db. For instance, findAll, findBySomeProperty, and so on.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Application Controller&lt;/code&gt;:
Main controller which receives the HTTP requests, handle and return a Result, which can be a simple html page, a WebSocket connection or JSON response. Available Methods:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Live tweets&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;GET        /                      controllers.Application.liveTweets()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Timeline&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;GET        /timeline              controllers.Application.timelineTweets()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pie chart&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;GET        /pie                   controllers.Application.pieTweets()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Web socket connection &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;GET        /ws-tweets             controllers.Application.wsTweets()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tweets by sentiment type&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;GET        /tweets/:sentiment     controllers.Application.tweets(sentiment: String)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tweets statistics&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;GET        /statistics            controllers.Application.statistics()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h3&gt;Application Installation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;OpenJDK1.5&lt;/code&gt;: sudo apt-get install openjdk-7-jdk&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Play Framework Installation&lt;/code&gt; - &lt;a href="https://www.playframework.com/documentation/2.3.x/Installing"&gt; https://www.playframework.com/documentation/2.3.x/Installing&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Import the project - IntelliJIDEA&lt;/code&gt; - &lt;a href="https://confluence.jetbrains.com/display/IntelliJIDEA/Play+Framework+2.0"&gt;https://confluence.jetbrains.com/display/IntelliJIDEA/Play+Framework+2.0&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;HP Vertica DB&lt;/code&gt; - &lt;a href="https://my.vertica.com/docs/5.0/PDF/Installation%20Guide.pdf"&gt;https://my.vertica.com/docs/5.0/PDF/Installation%20Guide.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Redis&lt;/code&gt; - &lt;a href="http://redis.io/topics/quickstart"&gt;http://redis.io/topics/quickstart&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;HP Idol Keys&lt;/code&gt;- &lt;a href="http://idolondemand.topcoder.com/"&gt;http://idolondemand.topcoder.com/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Place your keys in the application.conf file under twitter-analyst/conf/ folder&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;// HP IDOL Platform
hp.idol.analyze.sentiment.uri=&amp;quot;https://api.idolondemand.com/1/api/sync/analyzesentiment/v1?text=%TWEET%&amp;amp;language=%LANG%&amp;amp;apikey=&amp;lt;IDOL_API_KEY_HERE&amp;gt;&amp;quot;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Twitter API Keys&lt;/code&gt; - &lt;a href="http://www.androidhive.info/2012/09/android-twitter-oauth-connect-tutorial/"&gt;http://www.androidhive.info/2012/09/android-twitter-oauth-connect-tutorial/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Place your new keys in the application.conf file twitter-analyst/conf/ folder&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;// Twitter 4J OAuth
twitter.oauth.consumerKey=&amp;quot;&amp;quot;
twitter.oauth.consumerSecret=&amp;quot;&amp;quot;
twitter.oauth.accessToken=&amp;quot;&amp;quot;
twitter.oauth.accessTokenSecret=&amp;quot;&amp;quot;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="http://www.androidhive.info/2012/09/android-twitter-oauth-connect-tutorial/"&gt;this tutorial&lt;/a&gt; shows you how to get
the twitter keys and tokens.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Connecting to Vertica&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Make sure you are using the persistence.xml with PostgreSQLDialect and setting your default schema name, like&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;persistence&lt;/span&gt; &lt;span class="na"&gt;xmlns=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;http://java.sun.com/xml/ns/persistence&amp;quot;&lt;/span&gt;
             &lt;span class="na"&gt;xmlns:xsi=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;http://www.w3.org/2001/XMLSchema-instance&amp;quot;&lt;/span&gt;
             &lt;span class="na"&gt;xsi:schemaLocation=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;http://java.sun.com/xml/ns/persistence http://java.sun.com/xml/ns/persistence/persistence_2_0.xsd&amp;quot;&lt;/span&gt;
             &lt;span class="na"&gt;version=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;2.0&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;

    &lt;span class="nt"&gt;&amp;lt;persistence-unit&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;defaultPersistenceUnit&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;transaction-type=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;RESOURCE_LOCAL&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;provider&amp;gt;&lt;/span&gt;org.hibernate.ejb.HibernatePersistence&lt;span class="nt"&gt;&amp;lt;/provider&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;non-jta-data-source&amp;gt;&lt;/span&gt;DefaultDS&lt;span class="nt"&gt;&amp;lt;/non-jta-data-source&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;properties&amp;gt;&lt;/span&gt;
            &lt;span class="c"&gt;&amp;lt;!--&amp;lt;property name=&amp;quot;hibernate.hbm2ddl.auto&amp;quot; value=&amp;quot;none&amp;quot; /&amp;gt;--&amp;gt;&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;lt;property&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;hibernate.dialect&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;value=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;org.hibernate.dialect.PostgreSQLDialect&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt; // Postgre Dialect
            &lt;span class="nt"&gt;&amp;lt;property&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;show_sql&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;value=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;true&amp;quot;&lt;/span&gt; &lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;lt;property&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;format_sql&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;value=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;true&amp;quot;&lt;/span&gt; &lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;lt;property&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;use_sql_comments&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;value=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;true&amp;quot;&lt;/span&gt; &lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;lt;property&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;hibernate.default_schema&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;value=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;public&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;/properties&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;/persistence-unit&amp;gt;&lt;/span&gt;

&lt;span class="nt"&gt;&amp;lt;/persistence&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;After that, you need to include in the classpath the &lt;em&gt;vertica-jdk5-6.1.0-0.jar&lt;/em&gt; which is located at twitter-analyst/lib/
folder of the project.&lt;/p&gt;
&lt;p&gt;You need to update the Vertica IP address, db-name, schema-name into application.conf file under twitter-analyst/conf/.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;// VerticaDB
db.default.driver=com.vertica.jdbc.Driver //This is the driver class that we use to connect to Vertica.
db.default.url=&amp;quot;jdbc:vertica://&amp;lt;your.db.ip.addr&amp;gt;:5433/&amp;lt;db-name&amp;gt;&amp;quot;
db.default.user=dbadmin
db.default.password=&amp;quot;&amp;quot;
db.default.schema=&amp;quot;&amp;lt;schema-name&amp;gt;&amp;quot;
db.default.jndiName=DefaultDS
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Creating the Model&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Play Framework can apply your sql scripts to the database, however I could not use this feature. I got some issues with
the Postgre dialect here. So, in this case I suggest you to copy the file from&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;/twitter-analyst/conf/evolutions/VMart/ddl/1.sql&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and execute it directly in VerticaDB. Make sure your schema is the same that we use in these files, or just replace it with your schema name.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Loading the Data&lt;/code&gt;
Under the same folder you will find the dml/2.sql which contains 10K+ inserts of tweet with $HPQ tag, you can load it into your DB. Make sure the schema name is right.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Connecting to Redis&lt;/code&gt;
Just place the Redis IP address in the application.conf file&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;// Redis
redis.host=&amp;quot;&amp;lt;your.redis.ip.addr&amp;gt;&amp;quot;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Starting the Application&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After install the VerticaDB, Redis, Generate you keys for Idol and Twitter, Import the project, you just need
to go in &lt;code&gt;Run -&amp;gt; Run Play 2&lt;/code&gt; and wait for the application bootstrap. &lt;/p&gt;
&lt;p&gt;If you do not see any tweets in live tweets chart, just tweet in your own account using the tag $HPQ and you will see it in the live chart. Or just set the property 'tweet.analyst.tags' in application.conf file to use a trend hashtag and you will receive a tsunami of tweets.&lt;/p&gt;
&lt;p&gt;Here we have two examples of positive and negative tweets in the live tweets chart.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Live Positive Tweet" src="/images/live-positive-tweet.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The green dots are all tweets considered positive, which means they received a score &amp;gt; 0 from HP Sentiment Analysis API.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Live Negative Tweet" src="/images/live-negative-tweet.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The red dots are all tweets considered negative, score &amp;lt; 0.&lt;/li&gt;
&lt;li&gt;The gray dots are all tweets considered neutral, score = 0.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To see the sentiment score information and the tweets content you can navigate between the charts Live, Timeline and Pie. Each one of them will be reloaded completely from scratch every time you hit the page. Place the mouse over the dots to see the content.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Live: tweets analyzed in real time and placed in the chart.&lt;/li&gt;
&lt;li&gt;Timeline: all the tweets analyzed and stored in the db can be filtered by date range.&lt;/li&gt;
&lt;li&gt;Pie: Tweets sentiment share.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h3&gt;Final Thoughts&lt;/h3&gt;
&lt;p&gt;It was my first application using Play Framework, Akka and HP Idol API. I have to say it was really easy and fast to setup and create the app. The more important here was that I could use my time to focus on what features I would create instead of spending time on the minimum viable architecture. &lt;/p&gt;
&lt;p&gt;The Akka is a great framework to create microservices without pain. I would like to had developed unit and integration tests, mainly for the actors. Due to the time constraints, unfortunately, I was not able to do it. But I will. In a near future. I do want to see and learn how easy/complex is to test the actors and the message flow.&lt;/p&gt;
&lt;p&gt;The Sentiment API was another great tool. I really want explore the other APIs from HP Idol OnDemand, for sure. In this challenge I could see that we can create nice apps just plugin into different APIs around the world. We can get a ton of data from social media APIs (Twitter Stream, awesome!! your app receiving tweets near real time). Grab and send it to be processed by whatever open and free service you find out on the internet. Put together the pieces and wala!
You have a nice app running.&lt;/p&gt;
&lt;p&gt;I think that`s all people.&lt;/p&gt;
&lt;p&gt;Feel free to contribute, share your opinion about the design decisions and technologies.&lt;/p&gt;
&lt;p&gt;Thanks!&lt;/p&gt;
&lt;p&gt;&lt;a href="http://github.com/fforbeck/twitter-analyst"&gt;&lt;img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/38ef81f8aca64bb9a64448d0d70f1308ef5341ab/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[]`s,&lt;/p&gt;
&lt;p&gt;Felipe Forbeck.&lt;/p&gt;</summary><category term="play framework"></category><category term="hp-idol"></category><category term="sentiment analysis"></category></entry><entry><title>Engineering culture @ Spotify</title><link href="http://felipeforbeck.com/posts/2014/07/engineering-culture-spotify/" rel="alternate"></link><updated>2014-07-21T22:08:00-07:00</updated><author><name>Felipe Forbeck</name></author><id>tag:felipeforbeck.com,2014-07-21:posts/2014/07/engineering-culture-spotify/</id><summary type="html">&lt;div class="vimeo" align="center"&gt;&lt;iframe width="800" height="500" src="https://player.vimeo.com/video/85490944" frameborder="0"&gt;&lt;/iframe&gt;&lt;/div&gt;</summary><category term="software &amp; product development"></category><category term="spotify"></category></entry><entry><title>Revolution OS</title><link href="http://felipeforbeck.com/posts/2014/07/the-linux-story/" rel="alternate"></link><updated>2014-07-21T21:41:00-07:00</updated><author><name>Felipe Forbeck</name></author><id>tag:felipeforbeck.com,2014-07-21:posts/2014/07/the-linux-story/</id><summary type="html">&lt;ul class="simple"&gt;
&lt;li&gt;How does the Linux OS was created?&lt;/li&gt;
&lt;li&gt;Why the community work is so important and powerful?&lt;/li&gt;
&lt;li&gt;Why do you should care about free and open source software?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Take a look at this documentary, you can have the answer for this and many other questions.&lt;/p&gt;
&lt;div class="youtube" align="center"&gt;&lt;iframe width="800" height="500" src="https://www.youtube.com/embed/plMxWpXhqig" frameborder="0"&gt;&lt;/iframe&gt;&lt;/div&gt;</summary><category term="linux"></category><category term="open source"></category><category term="free software"></category></entry><entry><title>The Internet's Own Boy</title><link href="http://felipeforbeck.com/posts/2014/07/the-internets-own-boy/" rel="alternate"></link><updated>2014-07-21T21:10:00-07:00</updated><author><name>Felipe Forbeck</name></author><id>tag:felipeforbeck.com,2014-07-21:posts/2014/07/the-internets-own-boy/</id><summary type="html">&lt;p&gt;A great documentary about Aaron Swartz, a programmer and information activist. Helped in the development the RSS and was co-founder of Reddit. He fought for free information access and he was ridiculously overwhelmed by the system.&lt;/p&gt;
&lt;p&gt;hope he can inspire many people...&lt;/p&gt;
&lt;div class="youtube" align="center"&gt;&lt;iframe width="800" height="500" src="https://www.youtube.com/embed/ehxGPRgUvzc" frameborder="0"&gt;&lt;/iframe&gt;&lt;/div&gt;</summary><category term="information activism"></category><category term="hacktivism"></category><category term="free sharing"></category></entry></feed>